{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "2-2-3_Dataset_DataLoader.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u-masao/YutaroOgawa_pytorch_advanced/blob/master/2_objectdetection/2_SSD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UxUxPn-XMPZ"
      },
      "source": [
        "# はじめに\n",
        "\n",
        "『つくりながら学ぶ! PyTorchによる発展ディープラーニング』 のサンプルコードを Google Colaboratory で動作にしました。\n",
        "\n",
        "オリジナルリポジトリ\n",
        "\n",
        "https://github.com/YutaroOgawa/pytorch_advanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjjDdgVQ7to9",
        "outputId": "072955f3-6fea-459d-8ca3-c41c2d3b9e4d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkWPbTi9W0gM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a29a2a-423c-4c41-d7e1-4b76579dd6e3"
      },
      "source": [
        "! git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "! ln -s pytorch_advanced/2_objectdetection/data data\n",
        "! ln -s pytorch_advanced/2_objectdetection/utils utils\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch_advanced'...\n",
            "remote: Enumerating objects: 479, done.\u001b[K\n",
            "remote: Total 479 (delta 0), reused 0 (delta 0), pack-reused 479\u001b[K\n",
            "Receiving objects: 100% (479/479), 14.75 MiB | 7.00 MiB/s, done.\n",
            "Resolving deltas: 100% (259/259), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjnJVJjzW0d-"
      },
      "source": [
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87oM8kd1W0b2"
      },
      "source": [
        "# フォルダ「data」が存在しない場合は作成する\n",
        "data_dir = \"./data/\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fihGJydaW0Zi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "391fe8c9-b92e-47ef-a449-ad2d236a8ad4"
      },
      "source": [
        "\n",
        "# フォルダ「weights」が存在しない場合は作成する\n",
        "weights_dir = \"./drive/MyDrive/weights/\"\n",
        "if not os.path.exists(weights_dir):\n",
        "    os.mkdir(weights_dir)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-49c6d4f1b3ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mweights_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./drive/weights/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m: [Errno 95] Operation not supported: './drive/weights/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW5FD1y7W0Xc"
      },
      "source": [
        "# VOC2012のデータセットをここからダウンロードします\n",
        "# 時間がかかります（約15分）\n",
        "url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
        "target_path = os.path.join(data_dir, \"VOCtrainval_11-May-2012.tar\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)\n",
        "    \n",
        "    tar = tarfile.TarFile(target_path)  # tarファイルを読み込み\n",
        "    tar.extractall(data_dir)  # tarを解凍\n",
        "    tar.close()  # tarファイルをクローズ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBRwQWIYW0VV"
      },
      "source": [
        "# 学習済みのSSD用のVGGのパラメータをフォルダ「weights」にダウンロード\n",
        "# MIT License\n",
        "# Copyright (c) 2017 Max deGroot, Ellis Brown\n",
        "# https://github.com/amdegroot/ssd.pytorch\n",
        "    \n",
        "url = \"https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth\"\n",
        "target_path = os.path.join(weights_dir, \"vgg16_reducedfc.pth\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mhpM-fDW0TQ"
      },
      "source": [
        "# 学習済みのSSD300モデルをフォルダ「weights」にダウンロード\n",
        "# MIT License\n",
        "# Copyright (c) 2017 Max deGroot, Ellis Brown\n",
        "# https://github.com/amdegroot/ssd.pytorch\n",
        "\n",
        "url = \"https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth\"\n",
        "target_path = os.path.join(weights_dir, \"ssd300_mAP_77.43_v2.pth\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z23ImK2gW0Ql"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLC2UpJZWu88"
      },
      "source": [
        "SSDなど物体検出アルゴリズム用のDatasetとDataLoaderを作成します。\n",
        "\n",
        "VOC2012データセットを対象とします。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEYTI9ZDWu88"
      },
      "source": [
        "# 2.2 Datasetの実装\n",
        "\n",
        "1.\t物体検出で使用するDatasetクラスを作成できるようになる\n",
        "2.\tSSDの学習時のデータオーギュメンテーションで、何をしているのかを理解する\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAZ2Mx4UWu88"
      },
      "source": [
        "# 2.3 DataLoaderの実装\n",
        "\n",
        "1.\t物体検出で使用するDataLoaderクラスを作成できるようになる\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaMizDyAWu88"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx8tuY7iWu88"
      },
      "source": [
        "# パッケージのimport\n",
        "import os.path as osp\n",
        "import random\n",
        "# XMLをファイルやテキストから読み込んだり、加工したり、保存したりするためのライブラリ\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQnAGLqCWu88"
      },
      "source": [
        "# 乱数のシードを設定\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-O_bcxSWu89"
      },
      "source": [
        "画像データ、アノテーションデータへのファイルパスのリストを作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FznZNQS0Wu89"
      },
      "source": [
        "# 学習、検証の画像データとアノテーションデータへのファイルパスリストを作成する\n",
        "\n",
        "\n",
        "def make_datapath_list(rootpath):\n",
        "    \"\"\"\n",
        "    データへのパスを格納したリストを作成する。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rootpath : str\n",
        "        データフォルダへのパス\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ret : train_img_list, train_anno_list, val_img_list, val_anno_list\n",
        "        データへのパスを格納したリスト\n",
        "    \"\"\"\n",
        "\n",
        "    # 画像ファイルとアノテーションファイルへのパスのテンプレートを作成\n",
        "    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')\n",
        "    annopath_template = osp.join(rootpath, 'Annotations', '%s.xml')\n",
        "\n",
        "    # 訓練と検証、それぞれのファイルのID（ファイル名）を取得する\n",
        "    train_id_names = osp.join(rootpath + 'ImageSets/Main/train.txt')\n",
        "    val_id_names = osp.join(rootpath + 'ImageSets/Main/val.txt')\n",
        "\n",
        "    # 訓練データの画像ファイルとアノテーションファイルへのパスリストを作成\n",
        "    train_img_list = list()\n",
        "    train_anno_list = list()\n",
        "\n",
        "    for line in open(train_id_names):\n",
        "        file_id = line.strip()  # 空白スペースと改行を除去\n",
        "        img_path = (imgpath_template % file_id)  # 画像のパス\n",
        "        anno_path = (annopath_template % file_id)  # アノテーションのパス\n",
        "        train_img_list.append(img_path)  # リストに追加\n",
        "        train_anno_list.append(anno_path)  # リストに追加\n",
        "\n",
        "    # 検証データの画像ファイルとアノテーションファイルへのパスリストを作成\n",
        "    val_img_list = list()\n",
        "    val_anno_list = list()\n",
        "\n",
        "    for line in open(val_id_names):\n",
        "        file_id = line.strip()  # 空白スペースと改行を除去\n",
        "        img_path = (imgpath_template % file_id)  # 画像のパス\n",
        "        anno_path = (annopath_template % file_id)  # アノテーションのパス\n",
        "        val_img_list.append(img_path)  # リストに追加\n",
        "        val_anno_list.append(anno_path)  # リストに追加\n",
        "\n",
        "    return train_img_list, train_anno_list, val_img_list, val_anno_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFyz5uCyWu89"
      },
      "source": [
        "# ファイルパスのリストを作成\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
        "    rootpath)\n",
        "\n",
        "# 動作確認\n",
        "print(train_img_list[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-q3-YdfWu8-"
      },
      "source": [
        "xml形式のアノテーションデータをリストに変換する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvZHmJS5Wu8-"
      },
      "source": [
        "# 「XML形式のアノテーション」を、リスト形式に変換するクラス\n",
        "\n",
        "\n",
        "class Anno_xml2list(object):\n",
        "    \"\"\"\n",
        "    1枚の画像に対する「XML形式のアノテーションデータ」を、画像サイズで規格化してからリスト形式に変換する。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    classes : リスト\n",
        "        VOCのクラス名を格納したリスト\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes):\n",
        "\n",
        "        self.classes = classes\n",
        "\n",
        "    def __call__(self, xml_path, width, height):\n",
        "        \"\"\"\n",
        "        1枚の画像に対する「XML形式のアノテーションデータ」を、画像サイズで規格化してからリスト形式に変換する。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xml_path : str\n",
        "            xmlファイルへのパス。\n",
        "        width : int\n",
        "            対象画像の幅。\n",
        "        height : int\n",
        "            対象画像の高さ。\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ret : [[xmin, ymin, xmax, ymax, label_ind], ... ]\n",
        "            物体のアノテーションデータを格納したリスト。画像内に存在する物体数分のだけ要素を持つ。\n",
        "        \"\"\"\n",
        "\n",
        "        # 画像内の全ての物体のアノテーションをこのリストに格納します\n",
        "        ret = []\n",
        "\n",
        "        # xmlファイルを読み込む\n",
        "        xml = ET.parse(xml_path).getroot()\n",
        "\n",
        "        # 画像内にある物体（object）の数だけループする\n",
        "        for obj in xml.iter('object'):\n",
        "\n",
        "            # アノテーションで検知がdifficultに設定されているものは除外\n",
        "            difficult = int(obj.find('difficult').text)\n",
        "            if difficult == 1:\n",
        "                continue\n",
        "\n",
        "            # 1つの物体に対するアノテーションを格納するリスト\n",
        "            bndbox = []\n",
        "\n",
        "            name = obj.find('name').text.lower().strip()  # 物体名\n",
        "            bbox = obj.find('bndbox')  # バウンディングボックスの情報\n",
        "\n",
        "            # アノテーションの xmin, ymin, xmax, ymaxを取得し、0～1に規格化\n",
        "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
        "\n",
        "            for pt in (pts):\n",
        "                # VOCは原点が(1,1)なので1を引き算して（0, 0）に\n",
        "                cur_pixel = int(bbox.find(pt).text) - 1\n",
        "\n",
        "                # 幅、高さで規格化\n",
        "                if pt == 'xmin' or pt == 'xmax':  # x方向のときは幅で割算\n",
        "                    cur_pixel /= width\n",
        "                else:  # y方向のときは高さで割算\n",
        "                    cur_pixel /= height\n",
        "\n",
        "                bndbox.append(cur_pixel)\n",
        "\n",
        "            # アノテーションのクラス名のindexを取得して追加\n",
        "            label_idx = self.classes.index(name)\n",
        "            bndbox.append(label_idx)\n",
        "\n",
        "            # resに[xmin, ymin, xmax, ymax, label_ind]を足す\n",
        "            ret += [bndbox]\n",
        "\n",
        "        return np.array(ret)  # [[xmin, ymin, xmax, ymax, label_ind], ... ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cGuTqipWu8-"
      },
      "source": [
        "# 動作確認　\n",
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
        "               'cow', 'diningtable', 'dog', 'horse',\n",
        "               'motorbike', 'person', 'pottedplant',\n",
        "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "\n",
        "transform_anno = Anno_xml2list(voc_classes)\n",
        "\n",
        "# 画像の読み込み OpenCVを使用\n",
        "ind = 1\n",
        "image_file_path = val_img_list[ind]\n",
        "img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "height, width, channels = img.shape  # 画像のサイズを取得\n",
        "\n",
        "# アノテーションをリストで表示\n",
        "transform_anno(val_anno_list[ind], width, height)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i70G4dH3Wu8-"
      },
      "source": [
        "画像とアノテーションの前処理を行うクラスDataTransformを作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcRL9d7BWu8-"
      },
      "source": [
        "# フォルダ「utils」にあるdata_augumentation.pyからimport。\n",
        "# 入力画像の前処理をするクラス\n",
        "from utils.data_augumentation import Compose, ConvertFromInts, ToAbsoluteCoords, PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans\n",
        "\n",
        "\n",
        "class DataTransform():\n",
        "    \"\"\"\n",
        "    画像とアノテーションの前処理クラス。訓練と推論で異なる動作をする。\n",
        "    画像のサイズを300x300にする。\n",
        "    学習時はデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    input_size : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    color_mean : (B, G, R)\n",
        "        各色チャネルの平均値。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, color_mean):\n",
        "        self.data_transform = {\n",
        "            'train': Compose([\n",
        "                ConvertFromInts(),  # intをfloat32に変換\n",
        "                ToAbsoluteCoords(),  # アノテーションデータの規格化を戻す\n",
        "                PhotometricDistort(),  # 画像の色調などをランダムに変化\n",
        "                Expand(color_mean),  # 画像のキャンバスを広げる\n",
        "                RandomSampleCrop(),  # 画像内の部分をランダムに抜き出す\n",
        "                RandomMirror(),  # 画像を反転させる\n",
        "                ToPercentCoords(),  # アノテーションデータを0-1に規格化\n",
        "                Resize(input_size),  # 画像サイズをinput_size×input_sizeに変形\n",
        "                SubtractMeans(color_mean)  # BGRの色の平均値を引き算\n",
        "            ]),\n",
        "            'val': Compose([\n",
        "                ConvertFromInts(),  # intをfloatに変換\n",
        "                Resize(input_size),  # 画像サイズをinput_size×input_sizeに変形\n",
        "                SubtractMeans(color_mean)  # BGRの色の平均値を引き算\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase, boxes, labels):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase : 'train' or 'val'\n",
        "            前処理のモードを指定。\n",
        "        \"\"\"\n",
        "        return self.data_transform[phase](img, boxes, labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iQQhVoLWu8-"
      },
      "source": [
        "# 動作の確認\n",
        "\n",
        "# 1. 画像読み込み\n",
        "image_file_path = train_img_list[0]\n",
        "img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "height, width, channels = img.shape  # 画像のサイズを取得\n",
        "\n",
        "# 2. アノテーションをリストに\n",
        "transform_anno = Anno_xml2list(voc_classes)\n",
        "anno_list = transform_anno(train_anno_list[0], width, height)\n",
        "\n",
        "# 3. 元画像の表示\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "# 4. 前処理クラスの作成\n",
        "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
        "input_size = 300  # 画像のinputサイズを300×300にする\n",
        "transform = DataTransform(input_size, color_mean)\n",
        "\n",
        "# 5. train画像の表示\n",
        "phase = \"train\"\n",
        "img_transformed, boxes, labels = transform(\n",
        "    img, phase, anno_list[:, :4], anno_list[:, 4])\n",
        "plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 6. val画像の表示\n",
        "phase = \"val\"\n",
        "img_transformed, boxes, labels = transform(\n",
        "    img, phase, anno_list[:, :4], anno_list[:, 4])\n",
        "plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x5cQEGJWu8-"
      },
      "source": [
        "Datasetを作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWT_eucoWu8-"
      },
      "source": [
        "# VOC2012のDatasetを作成する\n",
        "\n",
        "\n",
        "class VOCDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    VOC2012のDatasetを作成するクラス。PyTorchのDatasetクラスを継承。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    img_list : リスト\n",
        "        画像のパスを格納したリスト\n",
        "    anno_list : リスト\n",
        "        アノテーションへのパスを格納したリスト\n",
        "    phase : 'train' or 'test'\n",
        "        学習か訓練かを設定する。\n",
        "    transform : object\n",
        "        前処理クラスのインスタンス\n",
        "    transform_anno : object\n",
        "        xmlのアノテーションをリストに変換するインスタンス\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_list, anno_list, phase, transform, transform_anno):\n",
        "        self.img_list = img_list\n",
        "        self.anno_list = anno_list\n",
        "        self.phase = phase  # train もしくは valを指定\n",
        "        self.transform = transform  # 画像の変形\n",
        "        self.transform_anno = transform_anno  # アノテーションデータをxmlからリストへ\n",
        "\n",
        "    def __len__(self):\n",
        "        '''画像の枚数を返す'''\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        前処理をした画像のテンソル形式のデータとアノテーションを取得\n",
        "        '''\n",
        "        im, gt, h, w = self.pull_item(index)\n",
        "        return im, gt\n",
        "\n",
        "    def pull_item(self, index):\n",
        "        '''前処理をした画像のテンソル形式のデータ、アノテーション、画像の高さ、幅を取得する'''\n",
        "\n",
        "        # 1. 画像読み込み\n",
        "        image_file_path = self.img_list[index]\n",
        "        img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "        height, width, channels = img.shape  # 画像のサイズを取得\n",
        "\n",
        "        # 2. xml形式のアノテーション情報をリストに\n",
        "        anno_file_path = self.anno_list[index]\n",
        "        anno_list = self.transform_anno(anno_file_path, width, height)\n",
        "\n",
        "        # 3. 前処理を実施\n",
        "        img, boxes, labels = self.transform(\n",
        "            img, self.phase, anno_list[:, :4], anno_list[:, 4])\n",
        "\n",
        "        # 色チャネルの順番がBGRになっているので、RGBに順番変更\n",
        "        # さらに（高さ、幅、色チャネル）の順を（色チャネル、高さ、幅）に変換\n",
        "        img = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
        "\n",
        "        # BBoxとラベルをセットにしたnp.arrayを作成、変数名「gt」はground truth（答え）の略称\n",
        "        gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
        "\n",
        "        return img, gt, height, width\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi4bIBumWu8-"
      },
      "source": [
        "# 動作確認\n",
        "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
        "input_size = 300  # 画像のinputサイズを300×300にする\n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "\n",
        "# データの取り出し例\n",
        "val_dataset.__getitem__(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDIz361eWu8_"
      },
      "source": [
        "DataLoaderを作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WcbBHnaWu8_"
      },
      "source": [
        "def od_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Datasetから取り出すアノテーションデータのサイズが画像ごとに異なります。\n",
        "    画像内の物体数が2個であれば(2, 5)というサイズですが、3個であれば（3, 5）など変化します。\n",
        "    この変化に対応したDataLoaderを作成するために、\n",
        "    カスタイマイズした、collate_fnを作成します。\n",
        "    collate_fnは、PyTorchでリストからmini-batchを作成する関数です。\n",
        "    ミニバッチ分の画像が並んでいるリスト変数batchに、\n",
        "    ミニバッチ番号を指定する次元を先頭に1つ追加して、リストの形を変形します。\n",
        "    \"\"\"\n",
        "\n",
        "    targets = []\n",
        "    imgs = []\n",
        "    for sample in batch:\n",
        "        imgs.append(sample[0])  # sample[0] は画像imgです\n",
        "        targets.append(torch.FloatTensor(sample[1]))  # sample[1] はアノテーションgtです\n",
        "\n",
        "    # imgsはミニバッチサイズのリストになっています\n",
        "    # リストの要素はtorch.Size([3, 300, 300])です。\n",
        "    # このリストをtorch.Size([batch_num, 3, 300, 300])のテンソルに変換します\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "    # targetsはアノテーションデータの正解であるgtのリストです。\n",
        "    # リストのサイズはミニバッチサイズです。\n",
        "    # リストtargetsの要素は [n, 5] となっています。\n",
        "    # nは画像ごとに異なり、画像内にある物体の数となります。\n",
        "    # 5は [xmin, ymin, xmax, ymax, class_index] です\n",
        "\n",
        "    return imgs, targets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQosFgk-Wu8_"
      },
      "source": [
        "# データローダーの作成\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\n",
        "\n",
        "# 辞書型変数にまとめる\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "\n",
        "# 動作の確認\n",
        "batch_iterator = iter(dataloaders_dict[\"val\"])  # イタレータに変換\n",
        "images, targets = next(batch_iterator)  # 1番目の要素を取り出す\n",
        "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
        "print(len(targets))\n",
        "print(targets[1].size())  # ミニバッチのサイズのリスト、各要素は[n, 5]、nは物体数\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh7JNkn3Wu8_"
      },
      "source": [
        "print(train_dataset.__len__())\n",
        "print(val_dataset.__len__())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHaEJ_CfWu8_"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RegXsOLL9hbA"
      },
      "source": [
        "SSDのネットワークモデルと順伝搬forward関数を作成します。\r\n",
        "\r\n",
        "# 2.4 ネットワークモデルの実装\r\n",
        "SSDのネットワークモデルを構築している4つのモジュールを把握する\r\n",
        "SSDのネットワークモデルを作成できるようになる\r\n",
        "SSDで使用する様々な大きさのデフォルトボックスの実装方法を理解する\r\n",
        "\r\n",
        "# 2.5 順伝搬関数の実装\r\n",
        "Non-Maximum Suppressionを理解する\r\n",
        "SSDの推論時に使用するDetectクラスの順伝搬を理解する\r\n",
        "SSDの順伝搬を実装できるようになる\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp_sL9Hq8y9K"
      },
      "source": [
        "# パッケージのimport\r\n",
        "from math import sqrt\r\n",
        "from itertools import product\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "from torch.autograd import Function\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.nn.init as init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiSeRuU-QlZs"
      },
      "source": [
        "vggモジュールを実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XnNpCfD8yx4"
      },
      "source": [
        "# 35層にわたる、vggモジュールを作成\r\n",
        "def make_vgg():\r\n",
        "    layers = []\r\n",
        "    in_channels = 3  # 色チャネル数\r\n",
        "\r\n",
        "    # vggモジュールで使用する畳み込み層やマックスプーリングのチャネル数\r\n",
        "    cfg = [64, 64, 'M', 128, 128, 'M', 256, 256,\r\n",
        "           256, 'MC', 512, 512, 512, 'M', 512, 512, 512]\r\n",
        "\r\n",
        "    for v in cfg:\r\n",
        "        if v == 'M':\r\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\r\n",
        "        elif v == 'MC':\r\n",
        "            # ceilは出力サイズを、計算結果（float）に対して、切り上げで整数にするモード\r\n",
        "            # デフォルトでは出力サイズを計算結果（float）に対して、切り下げで整数にするfloorモード\r\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\r\n",
        "        else:\r\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\r\n",
        "            layers += [conv2d, nn.ReLU(inplace=True)]\r\n",
        "            in_channels = v\r\n",
        "\r\n",
        "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\r\n",
        "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\r\n",
        "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\r\n",
        "    layers += [pool5, conv6,\r\n",
        "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\r\n",
        "    return nn.ModuleList(layers)\r\n",
        "\r\n",
        "\r\n",
        "# 動作確認\r\n",
        "vgg_test = make_vgg()\r\n",
        "print(vgg_test)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI0h5JGZQntF"
      },
      "source": [
        "extrasモジュールを実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-m5KuFz9waN"
      },
      "source": [
        "# 8層にわたる、extrasモジュールを作成\r\n",
        "def make_extras():\r\n",
        "    layers = []\r\n",
        "    in_channels = 1024  # vggモジュールから出力された、extraに入力される画像チャネル数\r\n",
        "\r\n",
        "    # extraモジュールの畳み込み層のチャネル数を設定するコンフィギュレーション\r\n",
        "    cfg = [256, 512, 128, 256, 128, 256, 128, 256]\r\n",
        "\r\n",
        "    layers += [nn.Conv2d(in_channels, cfg[0], kernel_size=(1))]\r\n",
        "    layers += [nn.Conv2d(cfg[0], cfg[1], kernel_size=(3), stride=2, padding=1)]\r\n",
        "    layers += [nn.Conv2d(cfg[1], cfg[2], kernel_size=(1))]\r\n",
        "    layers += [nn.Conv2d(cfg[2], cfg[3], kernel_size=(3), stride=2, padding=1)]\r\n",
        "    layers += [nn.Conv2d(cfg[3], cfg[4], kernel_size=(1))]\r\n",
        "    layers += [nn.Conv2d(cfg[4], cfg[5], kernel_size=(3))]\r\n",
        "    layers += [nn.Conv2d(cfg[5], cfg[6], kernel_size=(1))]\r\n",
        "    layers += [nn.Conv2d(cfg[6], cfg[7], kernel_size=(3))]\r\n",
        "    \r\n",
        "    # 活性化関数のReLUは今回はSSDモデルの順伝搬のなかで用意することにし、\r\n",
        "    # extraモジュールでは用意していません\r\n",
        "\r\n",
        "    return nn.ModuleList(layers)\r\n",
        "\r\n",
        "\r\n",
        "# 動作確認\r\n",
        "extras_test = make_extras()\r\n",
        "print(extras_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbtWo8EwQpkA"
      },
      "source": [
        "locモジュールとconfモジュールを実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziddEOZC91Cn"
      },
      "source": [
        "# デフォルトボックスのオフセットを出力するloc_layers、\r\n",
        "# デフォルトボックスに対する各クラスの信頼度confidenceを出力するconf_layersを作成\r\n",
        "\r\n",
        "\r\n",
        "def make_loc_conf(num_classes=21, bbox_aspect_num=[4, 6, 6, 6, 4, 4]):\r\n",
        "\r\n",
        "    loc_layers = []\r\n",
        "    conf_layers = []\r\n",
        "\r\n",
        "    # VGGの22層目、conv4_3（source1）に対する畳み込み層\r\n",
        "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[0]\r\n",
        "                             * 4, kernel_size=3, padding=1)]\r\n",
        "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[0]\r\n",
        "                              * num_classes, kernel_size=3, padding=1)]\r\n",
        "\r\n",
        "    # VGGの最終層（source2）に対する畳み込み層\r\n",
        "    loc_layers += [nn.Conv2d(1024, bbox_aspect_num[1]\r\n",
        "                             * 4, kernel_size=3, padding=1)]\r\n",
        "    conf_layers += [nn.Conv2d(1024, bbox_aspect_num[1]\r\n",
        "                              * num_classes, kernel_size=3, padding=1)]\r\n",
        "\r\n",
        "    # extraの（source3）に対する畳み込み層\r\n",
        "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[2]\r\n",
        "                             * 4, kernel_size=3, padding=1)]\r\n",
        "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[2]\r\n",
        "                              * num_classes, kernel_size=3, padding=1)]\r\n",
        "\r\n",
        "    # extraの（source4）に対する畳み込み層\r\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[3]\r\n",
        "                             * 4, kernel_size=3, padding=1)]\r\n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[3]\r\n",
        "                              * num_classes, kernel_size=3, padding=1)]\r\n",
        "\r\n",
        "    # extraの（source5）に対する畳み込み層\r\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[4]\r\n",
        "                             * 4, kernel_size=3, padding=1)]\r\n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[4]\r\n",
        "                              * num_classes, kernel_size=3, padding=1)]\r\n",
        "\r\n",
        "    # extraの（source6）に対する畳み込み層\r\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[5]\r\n",
        "                             * 4, kernel_size=3, padding=1)]\r\n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[5]\r\n",
        "                              * num_classes, kernel_size=3, padding=1)]\r\n",
        "\r\n",
        "    return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)\r\n",
        "\r\n",
        "\r\n",
        "# 動作確認\r\n",
        "loc_test, conf_test = make_loc_conf()\r\n",
        "print(loc_test)\r\n",
        "print(conf_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOnv4U7SQrhk"
      },
      "source": [
        "L2Norm層を実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb4HLsvY94i_"
      },
      "source": [
        "# convC4_3からの出力をscale=20のL2Normで正規化する層\r\n",
        "class L2Norm(nn.Module):\r\n",
        "    def __init__(self, input_channels=512, scale=20):\r\n",
        "        super(L2Norm, self).__init__()  # 親クラスのコンストラクタ実行\r\n",
        "        self.weight = nn.Parameter(torch.Tensor(input_channels))\r\n",
        "        self.scale = scale  # 係数weightの初期値として設定する値\r\n",
        "        self.reset_parameters()  # パラメータの初期化\r\n",
        "        self.eps = 1e-10\r\n",
        "\r\n",
        "    def reset_parameters(self):\r\n",
        "        '''結合パラメータを大きさscaleの値にする初期化を実行'''\r\n",
        "        init.constant_(self.weight, self.scale)  # weightの値がすべてscale（=20）になる\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        '''38×38の特徴量に対して、512チャネルにわたって2乗和のルートを求めた\r\n",
        "        38×38個の値を使用し、各特徴量を正規化してから係数をかけ算する層'''\r\n",
        "\r\n",
        "        # 各チャネルにおける38×38個の特徴量のチャネル方向の2乗和を計算し、\r\n",
        "        # さらにルートを求め、割り算して正規化する\r\n",
        "        # normのテンソルサイズはtorch.Size([batch_num, 1, 38, 38])になります\r\n",
        "        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\r\n",
        "        x = torch.div(x, norm)\r\n",
        "\r\n",
        "        # 係数をかける。係数はチャネルごとに1つで、512個の係数を持つ\r\n",
        "        # self.weightのテンソルサイズはtorch.Size([512])なので\r\n",
        "        # torch.Size([batch_num, 512, 38, 38])まで変形します\r\n",
        "        weights = self.weight.unsqueeze(\r\n",
        "            0).unsqueeze(2).unsqueeze(3).expand_as(x)\r\n",
        "        out = weights * x\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HURvrsuiQtLj"
      },
      "source": [
        "デフォルトボックスを実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOQZoN0n97nF"
      },
      "source": [
        "# デフォルトボックスを出力するクラス\r\n",
        "class DBox(object):\r\n",
        "    def __init__(self, cfg):\r\n",
        "        super(DBox, self).__init__()\r\n",
        "\r\n",
        "        # 初期設定\r\n",
        "        self.image_size = cfg['input_size']  # 画像サイズの300\r\n",
        "        # [38, 19, …] 各sourceの特徴量マップのサイズ\r\n",
        "        self.feature_maps = cfg['feature_maps']\r\n",
        "        self.num_priors = len(cfg[\"feature_maps\"])  # sourceの個数=6\r\n",
        "        self.steps = cfg['steps']  # [8, 16, …] DBoxのピクセルサイズ\r\n",
        "        \r\n",
        "        self.min_sizes = cfg['min_sizes']\r\n",
        "        # [30, 60, …] 小さい正方形のDBoxのピクセルサイズ（正確には面積）\r\n",
        "        \r\n",
        "        self.max_sizes = cfg['max_sizes']\r\n",
        "        # [60, 111, …] 大きい正方形のDBoxのピクセルサイズ（正確には面積）\r\n",
        "        \r\n",
        "        self.aspect_ratios = cfg['aspect_ratios']  # 長方形のDBoxのアスペクト比\r\n",
        "\r\n",
        "    def make_dbox_list(self):\r\n",
        "        '''DBoxを作成する'''\r\n",
        "        mean = []\r\n",
        "        # 'feature_maps': [38, 19, 10, 5, 3, 1]\r\n",
        "        for k, f in enumerate(self.feature_maps):\r\n",
        "            for i, j in product(range(f), repeat=2):  # fまでの数で2ペアの組み合わせを作る　f_P_2 個\r\n",
        "                # 特徴量の画像サイズ\r\n",
        "                # 300 / 'steps': [8, 16, 32, 64, 100, 300],\r\n",
        "                f_k = self.image_size / self.steps[k]\r\n",
        "\r\n",
        "                # DBoxの中心座標 x,y　ただし、0～1で規格化している\r\n",
        "                cx = (j + 0.5) / f_k\r\n",
        "                cy = (i + 0.5) / f_k\r\n",
        "\r\n",
        "                # アスペクト比1の小さいDBox [cx,cy, width, height]\r\n",
        "                # 'min_sizes': [30, 60, 111, 162, 213, 264]\r\n",
        "                s_k = self.min_sizes[k]/self.image_size\r\n",
        "                mean += [cx, cy, s_k, s_k]\r\n",
        "\r\n",
        "                # アスペクト比1の大きいDBox [cx,cy, width, height]\r\n",
        "                # 'max_sizes': [60, 111, 162, 213, 264, 315],\r\n",
        "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\r\n",
        "                mean += [cx, cy, s_k_prime, s_k_prime]\r\n",
        "\r\n",
        "                # その他のアスペクト比のdefBox [cx,cy, width, height]\r\n",
        "                for ar in self.aspect_ratios[k]:\r\n",
        "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\r\n",
        "                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\r\n",
        "\r\n",
        "        # DBoxをテンソルに変換 torch.Size([8732, 4])\r\n",
        "        output = torch.Tensor(mean).view(-1, 4)\r\n",
        "\r\n",
        "        # DBoxが画像の外にはみ出るのを防ぐため、大きさを最小0、最大1にする\r\n",
        "        output.clamp_(max=1, min=0)\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya7h-o9m994t"
      },
      "source": [
        "# 動作の確認\r\n",
        "\r\n",
        "# SSD300の設定\r\n",
        "ssd_cfg = {\r\n",
        "    'num_classes': 21,  # 背景クラスを含めた合計クラス数\r\n",
        "    'input_size': 300,  # 画像の入力サイズ\r\n",
        "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\r\n",
        "    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\r\n",
        "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\r\n",
        "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\r\n",
        "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\r\n",
        "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\r\n",
        "}\r\n",
        "\r\n",
        "# DBox作成\r\n",
        "dbox = DBox(ssd_cfg)\r\n",
        "dbox_list = dbox.make_dbox_list()\r\n",
        "\r\n",
        "# DBoxの出力を確認する\r\n",
        "pd.DataFrame(dbox_list.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHH8f2dPQw19"
      },
      "source": [
        "SSDクラスを実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN4RJcw2-AAk"
      },
      "source": [
        "# SSDクラスを作成する\r\n",
        "class SSD(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, phase, cfg):\r\n",
        "        super(SSD, self).__init__()\r\n",
        "\r\n",
        "        self.phase = phase  # train or inferenceを指定\r\n",
        "        self.num_classes = cfg[\"num_classes\"]  # クラス数=21\r\n",
        "\r\n",
        "        # SSDのネットワークを作る\r\n",
        "        self.vgg = make_vgg()\r\n",
        "        self.extras = make_extras()\r\n",
        "        self.L2Norm = L2Norm()\r\n",
        "        self.loc, self.conf = make_loc_conf(\r\n",
        "            cfg[\"num_classes\"], cfg[\"bbox_aspect_num\"])\r\n",
        "\r\n",
        "        # DBox作成\r\n",
        "        dbox = DBox(cfg)\r\n",
        "        self.dbox_list = dbox.make_dbox_list()\r\n",
        "\r\n",
        "        # 推論時はクラス「Detect」を用意します\r\n",
        "        if phase == 'inference':\r\n",
        "            self.detect = Detect()\r\n",
        "\r\n",
        "\r\n",
        "# 動作確認\r\n",
        "ssd_test = SSD(phase=\"train\", cfg=ssd_cfg)\r\n",
        "print(ssd_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0hAcAJm-CsV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5BZw9zu-Fec"
      },
      "source": [
        "2.5 順伝搬の実装\r\n",
        "\r\n",
        "関数decodeを実装する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2eCiSgl-GXU"
      },
      "source": [
        "# オフセット情報を使い、DBoxをBBoxに変換する関数\r\n",
        "\r\n",
        "\r\n",
        "def decode(loc, dbox_list):\r\n",
        "    \"\"\"\r\n",
        "    オフセット情報を使い、DBoxをBBoxに変換する。\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    loc:  [8732,4]\r\n",
        "        SSDモデルで推論するオフセット情報。\r\n",
        "    dbox_list: [8732,4]\r\n",
        "        DBoxの情報\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    boxes : [xmin, ymin, xmax, ymax]\r\n",
        "        BBoxの情報\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # DBoxは[cx, cy, width, height]で格納されている\r\n",
        "    # locも[Δcx, Δcy, Δwidth, Δheight]で格納されている\r\n",
        "\r\n",
        "    # オフセット情報からBBoxを求める\r\n",
        "    boxes = torch.cat((\r\n",
        "        dbox_list[:, :2] + loc[:, :2] * 0.1 * dbox_list[:, 2:],\r\n",
        "        dbox_list[:, 2:] * torch.exp(loc[:, 2:] * 0.2)), dim=1)\r\n",
        "    # boxesのサイズはtorch.Size([8732, 4])となります\r\n",
        "\r\n",
        "    # BBoxの座標情報を[cx, cy, width, height]から[xmin, ymin, xmax, ymax] に\r\n",
        "    boxes[:, :2] -= boxes[:, 2:] / 2  # 座標(xmin,ymin)へ変換\r\n",
        "    boxes[:, 2:] += boxes[:, :2]  # 座標(xmax,ymax)へ変換\r\n",
        "\r\n",
        "    return boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgWjrmQdQ30b"
      },
      "source": [
        "Non-Maximum Suppressionを行う関数を実装する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cNqg9UI-I1H"
      },
      "source": [
        "# Non-Maximum Suppressionを行う関数\r\n",
        "\r\n",
        "\r\n",
        "def nm_suppression(boxes, scores, overlap=0.45, top_k=200):\r\n",
        "    \"\"\"\r\n",
        "    Non-Maximum Suppressionを行う関数。\r\n",
        "    boxesのうち被り過ぎ（overlap以上）のBBoxを削除する。\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    boxes : [確信度閾値（0.01）を超えたBBox数,4]\r\n",
        "        BBox情報。\r\n",
        "    scores :[確信度閾値（0.01）を超えたBBox数]\r\n",
        "        confの情報\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    keep : リスト\r\n",
        "        confの降順にnmsを通過したindexが格納\r\n",
        "    count：int\r\n",
        "        nmsを通過したBBoxの数\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # returnのひな形を作成\r\n",
        "    count = 0\r\n",
        "    keep = scores.new(scores.size(0)).zero_().long()\r\n",
        "    # keep：torch.Size([確信度閾値を超えたBBox数])、要素は全部0\r\n",
        "\r\n",
        "    # 各BBoxの面積areaを計算\r\n",
        "    x1 = boxes[:, 0]\r\n",
        "    y1 = boxes[:, 1]\r\n",
        "    x2 = boxes[:, 2]\r\n",
        "    y2 = boxes[:, 3]\r\n",
        "    area = torch.mul(x2 - x1, y2 - y1)\r\n",
        "\r\n",
        "    # boxesをコピーする。後で、BBoxの被り度合いIOUの計算に使用する際のひな形として用意\r\n",
        "    tmp_x1 = boxes.new()\r\n",
        "    tmp_y1 = boxes.new()\r\n",
        "    tmp_x2 = boxes.new()\r\n",
        "    tmp_y2 = boxes.new()\r\n",
        "    tmp_w = boxes.new()\r\n",
        "    tmp_h = boxes.new()\r\n",
        "\r\n",
        "    # socreを昇順に並び変える\r\n",
        "    v, idx = scores.sort(0)\r\n",
        "\r\n",
        "    # 上位top_k個（200個）のBBoxのindexを取り出す（200個存在しない場合もある）\r\n",
        "    idx = idx[-top_k:]\r\n",
        "\r\n",
        "    # idxの要素数が0でない限りループする\r\n",
        "    while idx.numel() > 0:\r\n",
        "        i = idx[-1]  # 現在のconf最大のindexをiに\r\n",
        "\r\n",
        "        # keepの現在の最後にconf最大のindexを格納する\r\n",
        "        # このindexのBBoxと被りが大きいBBoxをこれから消去する\r\n",
        "        keep[count] = i\r\n",
        "        count += 1\r\n",
        "\r\n",
        "        # 最後のBBoxになった場合は、ループを抜ける\r\n",
        "        if idx.size(0) == 1:\r\n",
        "            break\r\n",
        "\r\n",
        "        # 現在のconf最大のindexをkeepに格納したので、idxをひとつ減らす\r\n",
        "        idx = idx[:-1]\r\n",
        "\r\n",
        "        # -------------------\r\n",
        "        # これからkeepに格納したBBoxと被りの大きいBBoxを抽出して除去する\r\n",
        "        # -------------------\r\n",
        "        # ひとつ減らしたidxまでのBBoxを、outに指定した変数として作成する\r\n",
        "        torch.index_select(x1, 0, idx, out=tmp_x1)\r\n",
        "        torch.index_select(y1, 0, idx, out=tmp_y1)\r\n",
        "        torch.index_select(x2, 0, idx, out=tmp_x2)\r\n",
        "        torch.index_select(y2, 0, idx, out=tmp_y2)\r\n",
        "\r\n",
        "        # すべてのBBoxに対して、現在のBBox=indexがiと被っている値までに設定(clamp)\r\n",
        "        tmp_x1 = torch.clamp(tmp_x1, min=x1[i])\r\n",
        "        tmp_y1 = torch.clamp(tmp_y1, min=y1[i])\r\n",
        "        tmp_x2 = torch.clamp(tmp_x2, max=x2[i])\r\n",
        "        tmp_y2 = torch.clamp(tmp_y2, max=y2[i])\r\n",
        "\r\n",
        "        # wとhのテンソルサイズをindexを1つ減らしたものにする\r\n",
        "        tmp_w.resize_as_(tmp_x2)\r\n",
        "        tmp_h.resize_as_(tmp_y2)\r\n",
        "\r\n",
        "        # clampした状態でのBBoxの幅と高さを求める\r\n",
        "        tmp_w = tmp_x2 - tmp_x1\r\n",
        "        tmp_h = tmp_y2 - tmp_y1\r\n",
        "\r\n",
        "        # 幅や高さが負になっているものは0にする\r\n",
        "        tmp_w = torch.clamp(tmp_w, min=0.0)\r\n",
        "        tmp_h = torch.clamp(tmp_h, min=0.0)\r\n",
        "\r\n",
        "        # clampされた状態での面積を求める\r\n",
        "        inter = tmp_w*tmp_h\r\n",
        "\r\n",
        "        # IoU = intersect部分 / (area(a) + area(b) - intersect部分)の計算\r\n",
        "        rem_areas = torch.index_select(area, 0, idx)  # 各BBoxの元の面積\r\n",
        "        union = (rem_areas - inter) + area[i]  # 2つのエリアの和（OR）の面積\r\n",
        "        IoU = inter/union\r\n",
        "\r\n",
        "        # IoUがoverlapより小さいidxのみを残す\r\n",
        "        idx = idx[IoU.le(overlap)]  # leはLess than or Equal toの処理をする演算です\r\n",
        "        # IoUがoverlapより大きいidxは、最初に選んでkeepに格納したidxと同じ物体に対してBBoxを囲んでいるため消去\r\n",
        "\r\n",
        "    # whileのループが抜けたら終了\r\n",
        "\r\n",
        "    return keep, count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBUf5RQTQ54q"
      },
      "source": [
        "Detectクラスを実装する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5HIP2W8-Lu0"
      },
      "source": [
        "# SSDの推論時にconfとlocの出力から、被りを除去したBBoxを出力する\r\n",
        "\r\n",
        "\r\n",
        "class Detect(Function):\r\n",
        "\r\n",
        "    def __init__(self, conf_thresh=0.01, top_k=200, nms_thresh=0.45):\r\n",
        "        self.softmax = nn.Softmax(dim=-1)  # confをソフトマックス関数で正規化するために用意\r\n",
        "        self.conf_thresh = conf_thresh  # confがconf_thresh=0.01より高いDBoxのみを扱う\r\n",
        "        self.top_k = top_k  # nm_supressionでconfの高いtop_k個を計算に使用する, top_k = 200\r\n",
        "        self.nms_thresh = nms_thresh  # nm_supressionでIOUがnms_thresh=0.45より大きいと、同一物体へのBBoxとみなす\r\n",
        "\r\n",
        "    def forward(self, loc_data, conf_data, dbox_list):\r\n",
        "        \"\"\"\r\n",
        "        順伝搬の計算を実行する。\r\n",
        "\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        loc_data:  [batch_num,8732,4]\r\n",
        "            オフセット情報。\r\n",
        "        conf_data: [batch_num, 8732,num_classes]\r\n",
        "            検出の確信度。\r\n",
        "        dbox_list: [8732,4]\r\n",
        "            DBoxの情報\r\n",
        "\r\n",
        "        Returns\r\n",
        "        -------\r\n",
        "        output : torch.Size([batch_num, 21, 200, 5])\r\n",
        "            （batch_num、クラス、confのtop200、BBoxの情報）\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # 各サイズを取得\r\n",
        "        num_batch = loc_data.size(0)  # ミニバッチのサイズ\r\n",
        "        num_dbox = loc_data.size(1)  # DBoxの数 = 8732\r\n",
        "        num_classes = conf_data.size(2)  # クラス数 = 21\r\n",
        "\r\n",
        "        # confはソフトマックスを適用して正規化する\r\n",
        "        conf_data = self.softmax(conf_data)\r\n",
        "\r\n",
        "        # 出力の型を作成する。テンソルサイズは[minibatch数, 21, 200, 5]\r\n",
        "        output = torch.zeros(num_batch, num_classes, self.top_k, 5)\r\n",
        "\r\n",
        "        # cof_dataを[batch_num,8732,num_classes]から[batch_num, num_classes,8732]に順番変更\r\n",
        "        conf_preds = conf_data.transpose(2, 1)\r\n",
        "\r\n",
        "        # ミニバッチごとのループ\r\n",
        "        for i in range(num_batch):\r\n",
        "\r\n",
        "            # 1. locとDBoxから修正したBBox [xmin, ymin, xmax, ymax] を求める\r\n",
        "            decoded_boxes = decode(loc_data[i], dbox_list)\r\n",
        "\r\n",
        "            # confのコピーを作成\r\n",
        "            conf_scores = conf_preds[i].clone()\r\n",
        "\r\n",
        "            # 画像クラスごとのループ（背景クラスのindexである0は計算せず、index=1から）\r\n",
        "            for cl in range(1, num_classes):\r\n",
        "\r\n",
        "                # 2.confの閾値を超えたBBoxを取り出す\r\n",
        "                # confの閾値を超えているかのマスクを作成し、\r\n",
        "                # 閾値を超えたconfのインデックスをc_maskとして取得\r\n",
        "                c_mask = conf_scores[cl].gt(self.conf_thresh)\r\n",
        "                # gtはGreater thanのこと。gtにより閾値を超えたものが1に、以下が0になる\r\n",
        "                # conf_scores:torch.Size([21, 8732])\r\n",
        "                # c_mask:torch.Size([8732])\r\n",
        "\r\n",
        "                # scoresはtorch.Size([閾値を超えたBBox数])\r\n",
        "                scores = conf_scores[cl][c_mask]\r\n",
        "\r\n",
        "                # 閾値を超えたconfがない場合、つまりscores=[]のときは、何もしない\r\n",
        "                if scores.nelement() == 0:  # nelementで要素数の合計を求める\r\n",
        "                    continue\r\n",
        "\r\n",
        "                # c_maskを、decoded_boxesに適用できるようにサイズを変更します\r\n",
        "                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\r\n",
        "                # l_mask:torch.Size([8732, 4])\r\n",
        "\r\n",
        "                # l_maskをdecoded_boxesに適応します\r\n",
        "                boxes = decoded_boxes[l_mask].view(-1, 4)\r\n",
        "                # decoded_boxes[l_mask]で1次元になってしまうので、\r\n",
        "                # viewで（閾値を超えたBBox数, 4）サイズに変形しなおす\r\n",
        "\r\n",
        "                # 3. Non-Maximum Suppressionを実施し、被っているBBoxを取り除く\r\n",
        "                ids, count = nm_suppression(\r\n",
        "                    boxes, scores, self.nms_thresh, self.top_k)\r\n",
        "                # ids：confの降順にNon-Maximum Suppressionを通過したindexが格納\r\n",
        "                # count：Non-Maximum Suppressionを通過したBBoxの数\r\n",
        "\r\n",
        "                # outputにNon-Maximum Suppressionを抜けた結果を格納\r\n",
        "                output[i, cl, :count] = torch.cat((scores[ids[:count]].unsqueeze(1),\r\n",
        "                                                   boxes[ids[:count]]), 1)\r\n",
        "\r\n",
        "        return output  # torch.Size([1, 21, 200, 5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnAsdZDyQ8P4"
      },
      "source": [
        "SSDクラスを実装する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKd1ouLY-O_s"
      },
      "source": [
        "# SSDクラスを作成する\r\n",
        "\r\n",
        "\r\n",
        "class SSD(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, phase, cfg):\r\n",
        "        super(SSD, self).__init__()\r\n",
        "\r\n",
        "        self.phase = phase  # train or inferenceを指定\r\n",
        "        self.num_classes = cfg[\"num_classes\"]  # クラス数=21\r\n",
        "\r\n",
        "        # SSDのネットワークを作る\r\n",
        "        self.vgg = make_vgg()\r\n",
        "        self.extras = make_extras()\r\n",
        "        self.L2Norm = L2Norm()\r\n",
        "        self.loc, self.conf = make_loc_conf(\r\n",
        "            cfg[\"num_classes\"], cfg[\"bbox_aspect_num\"])\r\n",
        "\r\n",
        "        # DBox作成\r\n",
        "        dbox = DBox(cfg)\r\n",
        "        self.dbox_list = dbox.make_dbox_list()\r\n",
        "\r\n",
        "        # 推論時はクラス「Detect」を用意します\r\n",
        "        if phase == 'inference':\r\n",
        "            self.detect = Detect()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        sources = list()  # locとconfへの入力source1～6を格納\r\n",
        "        loc = list()  # locの出力を格納\r\n",
        "        conf = list()  # confの出力を格納\r\n",
        "\r\n",
        "        # vggのconv4_3まで計算する\r\n",
        "        for k in range(23):\r\n",
        "            x = self.vgg[k](x)\r\n",
        "\r\n",
        "        # conv4_3の出力をL2Normに入力し、source1を作成、sourcesに追加\r\n",
        "        source1 = self.L2Norm(x)\r\n",
        "        sources.append(source1)\r\n",
        "\r\n",
        "        # vggを最後まで計算し、source2を作成、sourcesに追加\r\n",
        "        for k in range(23, len(self.vgg)):\r\n",
        "            x = self.vgg[k](x)\r\n",
        "\r\n",
        "        sources.append(x)\r\n",
        "\r\n",
        "        # extrasのconvとReLUを計算\r\n",
        "        # source3～6を、sourcesに追加\r\n",
        "        for k, v in enumerate(self.extras):\r\n",
        "            x = F.relu(v(x), inplace=True)\r\n",
        "            if k % 2 == 1:  # conv→ReLU→cov→ReLUをしたらsourceに入れる\r\n",
        "                sources.append(x)\r\n",
        "\r\n",
        "        # source1～6に、それぞれ対応する畳み込みを1回ずつ適用する\r\n",
        "        # zipでforループの複数のリストの要素を取得\r\n",
        "        # source1～6まであるので、6回ループが回る\r\n",
        "        for (x, l, c) in zip(sources, self.loc, self.conf):\r\n",
        "            # Permuteは要素の順番を入れ替え\r\n",
        "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\r\n",
        "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\r\n",
        "            # l(x)とc(x)で畳み込みを実行\r\n",
        "            # l(x)とc(x)の出力サイズは[batch_num, 4*アスペクト比の種類数, featuremapの高さ, featuremap幅]\r\n",
        "            # sourceによって、アスペクト比の種類数が異なり、面倒なので順番入れ替えて整える\r\n",
        "            # permuteで要素の順番を入れ替え、\r\n",
        "            # [minibatch数, featuremap数, featuremap数,4*アスペクト比の種類数]へ\r\n",
        "            # （注釈）\r\n",
        "            # torch.contiguous()はメモリ上で要素を連続的に配置し直す命令です。\r\n",
        "            # あとでview関数を使用します。\r\n",
        "            # このviewを行うためには、対象の変数がメモリ上で連続配置されている必要があります。\r\n",
        "\r\n",
        "        # さらにlocとconfの形を変形\r\n",
        "        # locのサイズは、torch.Size([batch_num, 34928])\r\n",
        "        # confのサイズはtorch.Size([batch_num, 183372])になる\r\n",
        "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\r\n",
        "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\r\n",
        "\r\n",
        "        # さらにlocとconfの形を整える\r\n",
        "        # locのサイズは、torch.Size([batch_num, 8732, 4])\r\n",
        "        # confのサイズは、torch.Size([batch_num, 8732, 21])\r\n",
        "        loc = loc.view(loc.size(0), -1, 4)\r\n",
        "        conf = conf.view(conf.size(0), -1, self.num_classes)\r\n",
        "\r\n",
        "        # 最後に出力する\r\n",
        "        output = (loc, conf, self.dbox_list)\r\n",
        "\r\n",
        "        if self.phase == \"inference\":  # 推論時\r\n",
        "            # クラス「Detect」のforwardを実行\r\n",
        "            # 返り値のサイズは torch.Size([batch_num, 21, 200, 5])\r\n",
        "            return self.detect(output[0], output[1], output[2])\r\n",
        "\r\n",
        "        else:  # 学習時\r\n",
        "            return output\r\n",
        "            # 返り値は(loc, conf, dbox_list)のタプル"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTw62orN-SD9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvfwTTU2-X1o"
      },
      "source": [
        "\r\n",
        "SSDネットワークモデルの損失関数を作成します。\r\n",
        "\r\n",
        "# 2.6 損失関数の実装\r\n",
        "jaccard係数を用いたmatch関数の動作を理解する\r\n",
        "Hard Negative Miningを理解する\r\n",
        "2種類の損失関数（SmoothL1Loss関数、交差エントロピー誤差関数）の働きを理解する\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9NHbUK8-drM"
      },
      "source": [
        "# パッケージのimport\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "# フォルダ「utils」にある関数matchを記述したmatch.pyからimport\r\n",
        "from utils.match import match"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XxheGqbRBc7"
      },
      "source": [
        "損失関数クラスMultiBoxLossを実装する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0wvZvqj-fpV"
      },
      "source": [
        "class MultiBoxLoss(nn.Module):\r\n",
        "    \"\"\"SSDの損失関数のクラスです。\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, jaccard_thresh=0.5, neg_pos=3, device='cpu'):\r\n",
        "        super(MultiBoxLoss, self).__init__()\r\n",
        "        self.jaccard_thresh = jaccard_thresh  # 0.5 関数matchのjaccard係数の閾値\r\n",
        "        self.negpos_ratio = neg_pos  # 3:1 Hard Negative Miningの負と正の比率\r\n",
        "        self.device = device  # CPUとGPUのいずれで計算するのか\r\n",
        "\r\n",
        "    def forward(self, predictions, targets):\r\n",
        "        \"\"\"\r\n",
        "        損失関数の計算。\r\n",
        "\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        predictions : SSD netの訓練時の出力(tuple)\r\n",
        "            (loc=torch.Size([num_batch, 8732, 4]), conf=torch.Size([num_batch, 8732, 21]), dbox_list=torch.Size [8732,4])。\r\n",
        "\r\n",
        "        targets : [num_batch, num_objs, 5]\r\n",
        "            5は正解のアノテーション情報[xmin, ymin, xmax, ymax, label_ind]を示す\r\n",
        "\r\n",
        "        Returns\r\n",
        "        -------\r\n",
        "        loss_l : テンソル\r\n",
        "            locの損失の値\r\n",
        "        loss_c : テンソル\r\n",
        "            confの損失の値\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # SSDモデルの出力がタプルになっているので、個々にばらす\r\n",
        "        loc_data, conf_data, dbox_list = predictions\r\n",
        "\r\n",
        "        # 要素数を把握\r\n",
        "        num_batch = loc_data.size(0)  # ミニバッチのサイズ\r\n",
        "        num_dbox = loc_data.size(1)  # DBoxの数 = 8732\r\n",
        "        num_classes = conf_data.size(2)  # クラス数 = 21\r\n",
        "\r\n",
        "        # 損失の計算に使用するものを格納する変数を作成\r\n",
        "        # conf_t_label：各DBoxに一番近い正解のBBoxのラベルを格納させる\r\n",
        "        # loc_t:各DBoxに一番近い正解のBBoxの位置情報を格納させる\r\n",
        "        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\r\n",
        "        loc_t = torch.Tensor(num_batch, num_dbox, 4).to(self.device)\r\n",
        "\r\n",
        "        # loc_tとconf_t_labelに、\r\n",
        "        # DBoxと正解アノテーションtargetsをmatchさせた結果を上書きする\r\n",
        "        for idx in range(num_batch):  # ミニバッチでループ\r\n",
        "\r\n",
        "            # 現在のミニバッチの正解アノテーションのBBoxとラベルを取得\r\n",
        "            truths = targets[idx][:, :-1].to(self.device)  # BBox\r\n",
        "            # ラベル [物体1のラベル, 物体2のラベル, …]\r\n",
        "            labels = targets[idx][:, -1].to(self.device)\r\n",
        "\r\n",
        "            # デフォルトボックスを新たな変数で用意\r\n",
        "            dbox = dbox_list.to(self.device)\r\n",
        "\r\n",
        "            # 関数matchを実行し、loc_tとconf_t_labelの内容を更新する\r\n",
        "            # （詳細）\r\n",
        "            # loc_t:各DBoxに一番近い正解のBBoxの位置情報が上書きされる\r\n",
        "            # conf_t_label：各DBoxに一番近いBBoxのラベルが上書きされる\r\n",
        "            # ただし、一番近いBBoxとのjaccard overlapが0.5より小さい場合は\r\n",
        "            # 正解BBoxのラベルconf_t_labelは背景クラスの0とする\r\n",
        "            variance = [0.1, 0.2]\r\n",
        "            # このvarianceはDBoxからBBoxに補正計算する際に使用する式の係数です\r\n",
        "            match(self.jaccard_thresh, truths, dbox,\r\n",
        "                  variance, labels, loc_t, conf_t_label, idx)\r\n",
        "\r\n",
        "        # ----------\r\n",
        "        # 位置の損失：loss_lを計算\r\n",
        "        # Smooth L1関数で損失を計算する。ただし、物体を発見したDBoxのオフセットのみを計算する\r\n",
        "        # ----------\r\n",
        "        # 物体を検出したBBoxを取り出すマスクを作成\r\n",
        "        pos_mask = conf_t_label > 0  # torch.Size([num_batch, 8732])\r\n",
        "\r\n",
        "        # pos_maskをloc_dataのサイズに変形\r\n",
        "        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\r\n",
        "\r\n",
        "        # Positive DBoxのloc_dataと、教師データloc_tを取得\r\n",
        "        loc_p = loc_data[pos_idx].view(-1, 4)\r\n",
        "        loc_t = loc_t[pos_idx].view(-1, 4)\r\n",
        "\r\n",
        "        # 物体を発見したPositive DBoxのオフセット情報loc_tの損失（誤差）を計算\r\n",
        "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\r\n",
        "\r\n",
        "        # ----------\r\n",
        "        # クラス予測の損失：loss_cを計算\r\n",
        "        # 交差エントロピー誤差関数で損失を計算する。ただし、背景クラスが正解であるDBoxが圧倒的に多いので、\r\n",
        "        # Hard Negative Miningを実施し、物体発見DBoxと背景クラスDBoxの比が1:3になるようにする。\r\n",
        "        # そこで背景クラスDBoxと予想したもののうち、損失が小さいものは、クラス予測の損失から除く\r\n",
        "        # ----------\r\n",
        "        batch_conf = conf_data.view(-1, num_classes)\r\n",
        "\r\n",
        "        # クラス予測の損失を関数を計算(reduction='none'にして、和をとらず、次元をつぶさない)\r\n",
        "        loss_c = F.cross_entropy(\r\n",
        "            batch_conf, conf_t_label.view(-1), reduction='none')\r\n",
        "\r\n",
        "        # -----------------\r\n",
        "        # これからNegative DBoxのうち、Hard Negative Miningで抽出するものを求めるマスクを作成します\r\n",
        "        # -----------------\r\n",
        "\r\n",
        "        # 物体発見したPositive DBoxの損失を0にする\r\n",
        "        # （注意）物体はlabelが1以上になっている。ラベル0は背景。\r\n",
        "        num_pos = pos_mask.long().sum(1, keepdim=True)  # ミニバッチごとの物体クラス予測の数\r\n",
        "        loss_c = loss_c.view(num_batch, -1)  # torch.Size([num_batch, 8732])\r\n",
        "        loss_c[pos_mask] = 0  # 物体を発見したDBoxは損失0とする\r\n",
        "\r\n",
        "        # Hard Negative Miningを実施する\r\n",
        "        # 各DBoxの損失の大きさloss_cの順位であるidx_rankを求める\r\n",
        "        _, loss_idx = loss_c.sort(1, descending=True)\r\n",
        "        _, idx_rank = loss_idx.sort(1)\r\n",
        "\r\n",
        "        # （注釈）\r\n",
        "        # 実装コードが特殊で直感的ではないです。\r\n",
        "        # 上記2行は、要は各DBoxに対して、損失の大きさが何番目なのかの情報を\r\n",
        "        # 変数idx_rankとして高速に取得したいというコードです。\r\n",
        "        #\r\n",
        "        # DBOXの損失値の大きい方から降順に並べ、DBoxの降順のindexをloss_idxに格納。\r\n",
        "        # 損失の大きさloss_cの順位であるidx_rankを求める。\r\n",
        "        # ここで、\r\n",
        "        # 降順になった配列indexであるloss_idxを、0から8732まで昇順に並べ直すためには、\r\n",
        "        # 何番目のloss_idxのインデックスをとってきたら良いのかを示すのが、idx_rankである。\r\n",
        "        # 例えば、\r\n",
        "        # idx_rankの要素0番目 = idx_rank[0]を求めるには、loss_idxの値が0の要素、\r\n",
        "        # つまりloss_idx[?}=0 の、?は何番かを求めることになる。ここで、? = idx_rank[0]である。\r\n",
        "        # いま、loss_idx[?]=0の0は、元のloss_cの要素の0番目という意味である。\r\n",
        "        # つまり?は、元のloss_cの要素0番目は、降順に並び替えられたloss_idxの何番目ですか\r\n",
        "        # を求めていることになり、 結果、\r\n",
        "        # ? = idx_rank[0] はloss_cの要素0番目が、降順の何番目かを示すことになる。\r\n",
        "\r\n",
        "        # 背景のDBoxの数num_negを決める。HardNegative Miningにより、\r\n",
        "        # 物体発見のDBoxの数num_posの3倍（self.negpos_ratio倍）とする。\r\n",
        "        # ただし、万が一、DBoxの数を超える場合は、DBoxの数を上限とする\r\n",
        "        num_neg = torch.clamp(num_pos*self.negpos_ratio, max=num_dbox)\r\n",
        "\r\n",
        "        # idx_rankは各DBoxの損失の大きさが上から何番目なのかが入っている\r\n",
        "        # 背景のDBoxの数num_negよりも、順位が低い（すなわち損失が大きい）DBoxを取るマスク作成\r\n",
        "        # torch.Size([num_batch, 8732])\r\n",
        "        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\r\n",
        "\r\n",
        "        # -----------------\r\n",
        "        # （終了）これからNegative DBoxのうち、Hard Negative Miningで抽出するものを求めるマスクを作成します\r\n",
        "        # -----------------\r\n",
        "\r\n",
        "        # マスクの形を整形し、conf_dataに合わせる\r\n",
        "        # pos_idx_maskはPositive DBoxのconfを取り出すマスクです\r\n",
        "        # neg_idx_maskはHard Negative Miningで抽出したNegative DBoxのconfを取り出すマスクです\r\n",
        "        # pos_mask：torch.Size([num_batch, 8732])→pos_idx_mask：torch.Size([num_batch, 8732, 21])\r\n",
        "        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\r\n",
        "        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\r\n",
        "\r\n",
        "        # conf_dataからposとnegだけを取り出してconf_hnmにする。形はtorch.Size([num_pos+num_neg, 21])\r\n",
        "        conf_hnm = conf_data[(pos_idx_mask+neg_idx_mask).gt(0)\r\n",
        "                             ].view(-1, num_classes)\r\n",
        "        # （注釈）gtは greater than (>)の略称。これでmaskが1のindexを取り出す。\r\n",
        "        # pos_idx_mask+neg_idx_maskは足し算だが、indexへのmaskをまとめているだけである。\r\n",
        "        # つまり、posであろうがnegであろうが、マスクが1のものを足し算で一つのリストにし、それをgtで取得\r\n",
        "\r\n",
        "        # 同様に教師データであるconf_t_labelからposとnegだけを取り出してconf_t_label_hnmに\r\n",
        "        # 形はtorch.Size([pos+neg])になる\r\n",
        "        conf_t_label_hnm = conf_t_label[(pos_mask+neg_mask).gt(0)]\r\n",
        "\r\n",
        "        # confidenceの損失関数を計算（要素の合計=sumを求める）\r\n",
        "        loss_c = F.cross_entropy(conf_hnm, conf_t_label_hnm, reduction='sum')\r\n",
        "\r\n",
        "        # 物体を発見したBBoxの数N（全ミニバッチの合計）で損失を割り算\r\n",
        "        N = num_pos.sum()\r\n",
        "        loss_l /= N\r\n",
        "        loss_c /= N\r\n",
        "\r\n",
        "        return loss_l, loss_c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nUQ3dPA-qkJ"
      },
      "source": [
        "# 2.7 学習と検証の実施\r\n",
        "\r\n",
        "本ファイルでは、SSDの学習と検証の実施を行います。手元のマシンで動作を確認後、AWSのGPUマシンで計算します。\r\n",
        "p2.xlargeで約6時間かかります。\r\n",
        "\r\n",
        "学習目標\r\n",
        "SSDの学習を実装できるようになる\r\n",
        "事前準備\r\n",
        "フォルダ「utils」のssd_model.pyをします\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3WEUCKK-xO9"
      },
      "source": [
        "# パッケージのimport\r\n",
        "import os.path as osp\r\n",
        "import random\r\n",
        "import time\r\n",
        "\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.init as init\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data as data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydFwKI5y_IVs"
      },
      "source": [
        "# 乱数のシードを設定\r\n",
        "torch.manual_seed(1234)\r\n",
        "np.random.seed(1234)\r\n",
        "random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwCxVtMxA6sP"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(\"使用デバイス：\", device)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgRvs7i7RGkR"
      },
      "source": [
        "DatasetとDataLoaderを作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrNG9eEFA8uB"
      },
      "source": [
        "from utils.ssd_model import make_datapath_list, VOCDataset, DataTransform, Anno_xml2list, od_collate_fn\r\n",
        "\r\n",
        "\r\n",
        "# ファイルパスのリストを取得\r\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\r\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\r\n",
        "    rootpath)\r\n",
        "\r\n",
        "# Datasetを作成\r\n",
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\r\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair',\r\n",
        "               'cow', 'diningtable', 'dog', 'horse',\r\n",
        "               'motorbike', 'person', 'pottedplant',\r\n",
        "               'sheep', 'sofa', 'train', 'tvmonitor']\r\n",
        "color_mean = (104, 117, 123)  # (BGR)の色の平均値\r\n",
        "input_size = 300  # 画像のinputサイズを300×300にする\r\n",
        "\r\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\r\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\r\n",
        "\r\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\r\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\r\n",
        "\r\n",
        "\r\n",
        "# DataLoaderを作成する\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "train_dataloader = data.DataLoader(\r\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\r\n",
        "\r\n",
        "val_dataloader = data.DataLoader(\r\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\r\n",
        "\r\n",
        "# 辞書オブジェクトにまとめる\r\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm4YwemMRKO0"
      },
      "source": [
        "ネットワークモデルの作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvkEA9BHA_Fm"
      },
      "source": [
        "from utils.ssd_model import SSD\r\n",
        "\r\n",
        "# SSD300の設定\r\n",
        "ssd_cfg = {\r\n",
        "    'num_classes': 21,  # 背景クラスを含めた合計クラス数\r\n",
        "    'input_size': 300,  # 画像の入力サイズ\r\n",
        "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\r\n",
        "    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\r\n",
        "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\r\n",
        "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\r\n",
        "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\r\n",
        "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\r\n",
        "}\r\n",
        "\r\n",
        "# SSDネットワークモデル\r\n",
        "net = SSD(phase=\"train\", cfg=ssd_cfg)\r\n",
        "\r\n",
        "# SSDの初期の重みを設定\r\n",
        "# ssdのvgg部分に重みをロードする\r\n",
        "vgg_weights = torch.load(weights_dir + '/vgg16_reducedfc.pth')\r\n",
        "net.vgg.load_state_dict(vgg_weights)\r\n",
        "\r\n",
        "# ssdのその他のネットワークの重みはHeの初期値で初期化\r\n",
        "\r\n",
        "\r\n",
        "def weights_init(m):\r\n",
        "    if isinstance(m, nn.Conv2d):\r\n",
        "        init.kaiming_normal_(m.weight.data)\r\n",
        "        if m.bias is not None:  # バイアス項がある場合\r\n",
        "            nn.init.constant_(m.bias, 0.0)\r\n",
        "\r\n",
        "\r\n",
        "# Heの初期値を適用\r\n",
        "net.extras.apply(weights_init)\r\n",
        "net.loc.apply(weights_init)\r\n",
        "net.conf.apply(weights_init)\r\n",
        "\r\n",
        "# GPUが使えるかを確認\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(\"使用デバイス：\", device)\r\n",
        "\r\n",
        "print('ネットワーク設定完了：学習済みの重みをロードしました')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVFpUK7gRMb9"
      },
      "source": [
        "損失関数と最適化手法を定義する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f1fS7qnBB5W"
      },
      "source": [
        "from utils.ssd_model import MultiBoxLoss\r\n",
        "\r\n",
        "# 損失関数の設定\r\n",
        "criterion = MultiBoxLoss(jaccard_thresh=0.5, neg_pos=3, device=device)\r\n",
        "\r\n",
        "# 最適化手法の設定\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-3,\r\n",
        "                      momentum=0.9, weight_decay=5e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iAc-I4GRNsh"
      },
      "source": [
        "学習・検証を実施する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd8wVVtSBDy3"
      },
      "source": [
        "# モデルを学習させる関数を作成\r\n",
        "\r\n",
        "\r\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\r\n",
        "\r\n",
        "    # GPUが使えるかを確認\r\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "    print(\"使用デバイス：\", device)\r\n",
        "\r\n",
        "    # ネットワークをGPUへ\r\n",
        "    net.to(device)\r\n",
        "\r\n",
        "    # ネットワークがある程度固定であれば、高速化させる\r\n",
        "    torch.backends.cudnn.benchmark = True\r\n",
        "\r\n",
        "    # イテレーションカウンタをセット\r\n",
        "    iteration = 1\r\n",
        "    epoch_train_loss = 0.0  # epochの損失和\r\n",
        "    epoch_val_loss = 0.0  # epochの損失和\r\n",
        "    logs = []\r\n",
        "\r\n",
        "    # epochのループ\r\n",
        "    for epoch in range(num_epochs+1):\r\n",
        "\r\n",
        "        # 開始時刻を保存\r\n",
        "        t_epoch_start = time.time()\r\n",
        "        t_iter_start = time.time()\r\n",
        "\r\n",
        "        print('-------------')\r\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\r\n",
        "        print('-------------')\r\n",
        "\r\n",
        "        # epochごとの訓練と検証のループ\r\n",
        "        for phase in ['train', 'val']:\r\n",
        "            if phase == 'train':\r\n",
        "                net.train()  # モデルを訓練モードに\r\n",
        "                print('（train）')\r\n",
        "            else:\r\n",
        "                if((epoch+1) % 10 == 0):\r\n",
        "                    net.eval()   # モデルを検証モードに\r\n",
        "                    print('-------------')\r\n",
        "                    print('（val）')\r\n",
        "                else:\r\n",
        "                    # 検証は10回に1回だけ行う\r\n",
        "                    continue\r\n",
        "\r\n",
        "            # データローダーからminibatchずつ取り出すループ\r\n",
        "            for images, targets in dataloaders_dict[phase]:\r\n",
        "\r\n",
        "                # GPUが使えるならGPUにデータを送る\r\n",
        "                images = images.to(device)\r\n",
        "                targets = [ann.to(device)\r\n",
        "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\r\n",
        "\r\n",
        "                # optimizerを初期化\r\n",
        "                optimizer.zero_grad()\r\n",
        "\r\n",
        "                # 順伝搬（forward）計算\r\n",
        "                with torch.set_grad_enabled(phase == 'train'):\r\n",
        "                    # 順伝搬（forward）計算\r\n",
        "                    outputs = net(images)\r\n",
        "\r\n",
        "                    # 損失の計算\r\n",
        "                    loss_l, loss_c = criterion(outputs, targets)\r\n",
        "                    loss = loss_l + loss_c\r\n",
        "\r\n",
        "                    # 訓練時はバックプロパゲーション\r\n",
        "                    if phase == 'train':\r\n",
        "                        loss.backward()  # 勾配の計算\r\n",
        "\r\n",
        "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\r\n",
        "                        nn.utils.clip_grad_value_(\r\n",
        "                            net.parameters(), clip_value=2.0)\r\n",
        "\r\n",
        "                        optimizer.step()  # パラメータ更新\r\n",
        "\r\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\r\n",
        "                            t_iter_finish = time.time()\r\n",
        "                            duration = t_iter_finish - t_iter_start\r\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\r\n",
        "                                iteration, loss.item(), duration))\r\n",
        "                            t_iter_start = time.time()\r\n",
        "\r\n",
        "                        epoch_train_loss += loss.item()\r\n",
        "                        iteration += 1\r\n",
        "\r\n",
        "                    # 検証時\r\n",
        "                    else:\r\n",
        "                        epoch_val_loss += loss.item()\r\n",
        "\r\n",
        "        # epochのphaseごとのlossと正解率\r\n",
        "        t_epoch_finish = time.time()\r\n",
        "        print('-------------')\r\n",
        "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\r\n",
        "            epoch+1, epoch_train_loss, epoch_val_loss))\r\n",
        "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\r\n",
        "        t_epoch_start = time.time()\r\n",
        "\r\n",
        "        # ログを保存\r\n",
        "        log_epoch = {'epoch': epoch+1,\r\n",
        "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\r\n",
        "        logs.append(log_epoch)\r\n",
        "        df = pd.DataFrame(logs)\r\n",
        "        df.to_csv(weights_dir+\"/log_output.csv\")\r\n",
        "\r\n",
        "        epoch_train_loss = 0.0  # epochの損失和\r\n",
        "        epoch_val_loss = 0.0  # epochの損失和\r\n",
        "\r\n",
        "        # ネットワークを保存する\r\n",
        "        if ((epoch+1) % 10 == 0):\r\n",
        "            torch.save(net.state_dict(), weights_dir+'/ssd300_' +\r\n",
        "                       str(epoch+1) + '.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEZP26i-BGjG"
      },
      "source": [
        "# 学習・検証を実行する\r\n",
        "num_epochs= 10  \r\n",
        "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8knzNtSjBKOO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ4WV1hJBZ9K"
      },
      "source": [
        "# 2.8 推論の実施\r\n",
        "本ファイルでは、学習させたSSDで物体検出を行います。\r\n",
        "\r\n",
        "学習目標\r\n",
        "SSDの推論を実装できるようになる\r\n",
        "\r\n",
        "事前準備\r\n",
        "学習させた重みパラメータを用意\r\n",
        "フォルダ「utils」のssd_predict_show.pyを使用します\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqUSI535BceN"
      },
      "source": [
        "import cv2  # OpenCVライブラリ\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNbtuus0RaZr"
      },
      "source": [
        "推論を実行する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6uFUAoqBfbm"
      },
      "source": [
        "from utils.ssd_model import SSD\r\n",
        "\r\n",
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\r\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair',\r\n",
        "               'cow', 'diningtable', 'dog', 'horse',\r\n",
        "               'motorbike', 'person', 'pottedplant',\r\n",
        "               'sheep', 'sofa', 'train', 'tvmonitor']\r\n",
        "\r\n",
        "# SSD300の設定\r\n",
        "ssd_cfg = {\r\n",
        "    'num_classes': 21,  # 背景クラスを含めた合計クラス数\r\n",
        "    'input_size': 300,  # 画像の入力サイズ\r\n",
        "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\r\n",
        "    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\r\n",
        "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\r\n",
        "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\r\n",
        "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\r\n",
        "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\r\n",
        "}\r\n",
        "\r\n",
        "# SSDネットワークモデル\r\n",
        "net = SSD(phase=\"inference\", cfg=ssd_cfg)\r\n",
        "\r\n",
        "# SSDの学習済みの重みを設定\r\n",
        "net_weights = torch.load(weights_dir+'/ssd300_50.pth',\r\n",
        "                         map_location={'cuda:0': 'cpu'})\r\n",
        "\r\n",
        "#net_weights = torch.load(weights_dir+'/ssd300_mAP_77.43_v2.pth',\r\n",
        "#                         map_location={'cuda:0': 'cpu'})\r\n",
        "\r\n",
        "net.load_state_dict(net_weights)\r\n",
        "\r\n",
        "print('ネットワーク設定完了：学習済みの重みをロードしました')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqC5X8LeBhWl"
      },
      "source": [
        "from utils.ssd_model import DataTransform\r\n",
        "\r\n",
        "# 1. 画像読み込み\r\n",
        "image_file_path = \"./data/cowboy-757575_640.jpg\"\r\n",
        "img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\r\n",
        "height, width, channels = img.shape  # 画像のサイズを取得\r\n",
        "\r\n",
        "# 2. 元画像の表示\r\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# 3. 前処理クラスの作成\r\n",
        "color_mean = (104, 117, 123)  # (BGR)の色の平均値\r\n",
        "input_size = 300  # 画像のinputサイズを300×300にする\r\n",
        "transform = DataTransform(input_size, color_mean)\r\n",
        "\r\n",
        "# 4. 前処理\r\n",
        "phase = \"val\"\r\n",
        "img_transformed, boxes, labels = transform(\r\n",
        "    img, phase, \"\", \"\")  # アノテーションはないので、\"\"にする\r\n",
        "img = torch.from_numpy(img_transformed[:, :, (2, 1, 0)]).permute(2, 0, 1)\r\n",
        "\r\n",
        "# 5. SSDで予測\r\n",
        "net.eval()  # ネットワークを推論モードへ\r\n",
        "x = img.unsqueeze(0)  # ミニバッチ化：torch.Size([1, 3, 300, 300])\r\n",
        "detections = net(x)\r\n",
        "\r\n",
        "print(detections.shape)\r\n",
        "print(detections)\r\n",
        "\r\n",
        "# output : torch.Size([batch_num, 21, 200, 5])\r\n",
        "#  =（batch_num、クラス、confのtop200、規格化されたBBoxの情報）\r\n",
        "#   規格化されたBBoxの情報（確信度、xmin, ymin, xmax, ymax）\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcPdClnvBjPm"
      },
      "source": [
        "# 画像に対する予測\r\n",
        "from utils.ssd_predict_show import SSDPredictShow\r\n",
        "\r\n",
        "# ファイルパス\r\n",
        "image_file_path = \"./data/cowboy-757575_640.jpg\"\r\n",
        "\r\n",
        "# 予測と、予測結果を画像で描画する\r\n",
        "ssd = SSDPredictShow(eval_categories=voc_classes, net=net)\r\n",
        "ssd.show(image_file_path, data_confidence_level=0.6)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTaKxzrvBnP7"
      },
      "source": [
        "# 2.8 推論の実施の付録。学習と検証のDataLoaderに実施する\r\n",
        "学習させたSSDで物体検出を行います。\r\n",
        "\r\n",
        "VOC2012の訓練データセットと検証データセットに対して、学習済みSSDの推論を実施し、推論結果と正しい答えであるアノテーションデータの両方を表示させるファイルです。\r\n",
        "\r\n",
        "学習させたSSDモデルが正しいアノテーションデータとどれくらい近いのかなどを確認したいケースでは、こちらもご使用ください。\r\n",
        "\r\n",
        "事前準備\r\n",
        "フォルダ「utils」に2.3～2.7までで実装した内容をまとめたssd_model.pyがあることを確認してください\r\n",
        "学習させた重みパラメータを用意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7nzklDABlCF"
      },
      "source": [
        "import cv2  # OpenCVライブラリ\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "\r\n",
        "%matplotlib inline\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmOKUxXRBsrT"
      },
      "source": [
        "推論用の関数とクラスを作成する\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sODRwFitBrYt"
      },
      "source": [
        "def ssd_predict(img_index, img_list, dataset, net=None, dataconfidence_level=0.5):\r\n",
        "    \"\"\"\r\n",
        "    SSDで予測させる関数。\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    img_index:  int\r\n",
        "        データセット内の予測対象画像のインデックス。\r\n",
        "    img_list: list\r\n",
        "        画像のファイルパスのリスト\r\n",
        "    dataset: PyTorchのDataset\r\n",
        "        画像のDataset\r\n",
        "    net: PyTorchのNetwork\r\n",
        "        学習させたSSDネットワーク\r\n",
        "    dataconfidence_level: float\r\n",
        "        予測で発見とする確信度の閾値\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    rgb_img, true_bbox, true_label_index, predict_bbox, pre_dict_label_index, scores\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # rgbの画像データを取得\r\n",
        "    image_file_path = img_list[img_index]\r\n",
        "    img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\r\n",
        "    height, width, channels = img.shape  # 画像のサイズを取得\r\n",
        "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n",
        "\r\n",
        "    # 正解のBBoxを取得\r\n",
        "    im, gt = dataset.__getitem__(img_index)\r\n",
        "    true_bbox = gt[:, 0:4] * [width, height, width, height]\r\n",
        "    true_label_index = gt[:, 4].astype(int)\r\n",
        "\r\n",
        "    # SSDで予測\r\n",
        "    net.eval()  # ネットワークを推論モードへ\r\n",
        "    x = im.unsqueeze(0)  # ミニバッチ化：torch.Size([1, 3, 300, 300])\r\n",
        "    detections = net(x)\r\n",
        "    # detectionsの形は、torch.Size([1, 21, 200, 5])  ※200はtop_kの値\r\n",
        "\r\n",
        "    # confidence_levelが基準以上を取り出す\r\n",
        "    predict_bbox = []\r\n",
        "    pre_dict_label_index = []\r\n",
        "    scores = []\r\n",
        "    detections = detections.cpu().detach().numpy()\r\n",
        "\r\n",
        "    # 条件以上の値を抽出\r\n",
        "    find_index = np.where(detections[:, 0:, :, 0] >= dataconfidence_level)\r\n",
        "    detections = detections[find_index]\r\n",
        "    for i in range(len(find_index[1])):  # 抽出した物体数分ループを回す\r\n",
        "        if (find_index[1][i]) > 0:  # 背景クラスでないもの\r\n",
        "            sc = detections[i][0]  # 確信度\r\n",
        "            bbox = detections[i][1:] * [width, height, width, height]\r\n",
        "            lable_ind = find_index[1][i]-1  # find_indexはミニバッチ数、クラス、topのtuple\r\n",
        "            # （注釈）\r\n",
        "            # 背景クラスが0なので1を引く\r\n",
        "\r\n",
        "            # 返り値のリストに追加\r\n",
        "            predict_bbox.append(bbox)\r\n",
        "            pre_dict_label_index.append(lable_ind)\r\n",
        "            scores.append(sc)\r\n",
        "\r\n",
        "    return rgb_img, true_bbox, true_label_index, predict_bbox, pre_dict_label_index, scores\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zKXXju4BvHy"
      },
      "source": [
        "def vis_bbox(rgb_img, bbox, label_index, scores, label_names):\r\n",
        "    \"\"\"\r\n",
        "    物体検出の予測結果を画像で表示させる関数。\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    rgb_img:rgbの画像\r\n",
        "        対象の画像データ\r\n",
        "    bbox: list\r\n",
        "        物体のBBoxのリスト\r\n",
        "    label_index: list\r\n",
        "        物体のラベルへのインデックス\r\n",
        "    scores: list\r\n",
        "        物体の確信度。\r\n",
        "    label_names: list\r\n",
        "        ラベル名の配列\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    なし。rgb_imgに物体検出結果が加わった画像が表示される。\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # 枠の色の設定\r\n",
        "    num_classes = len(label_names)  # クラス数（背景のぞく）\r\n",
        "    colors = plt.cm.hsv(np.linspace(0, 1, num_classes)).tolist()\r\n",
        "\r\n",
        "    # 画像の表示\r\n",
        "    plt.figure(figsize=(10, 10))\r\n",
        "    plt.imshow(rgb_img)\r\n",
        "    currentAxis = plt.gca()\r\n",
        "\r\n",
        "    # BBox分のループ\r\n",
        "    for i, bb in enumerate(bbox):\r\n",
        "\r\n",
        "        # ラベル名\r\n",
        "        label_name = label_names[label_index[i]]\r\n",
        "        color = colors[label_index[i]]  # クラスごとに別の色の枠を与える\r\n",
        "\r\n",
        "        # 枠につけるラベル　例：person;0.72　\r\n",
        "        if scores is not None:\r\n",
        "            sc = scores[i]\r\n",
        "            display_txt = '%s: %.2f' % (label_name, sc)\r\n",
        "        else:\r\n",
        "            display_txt = '%s: ans' % (label_name)\r\n",
        "\r\n",
        "        # 枠の座標\r\n",
        "        xy = (bb[0], bb[1])\r\n",
        "        width = bb[2] - bb[0]\r\n",
        "        height = bb[3] - bb[1]\r\n",
        "\r\n",
        "        # 長方形を描画する\r\n",
        "        currentAxis.add_patch(plt.Rectangle(\r\n",
        "            xy, width, height, fill=False, edgecolor=color, linewidth=2))\r\n",
        "\r\n",
        "        # 長方形の枠の左上にラベルを描画する\r\n",
        "        currentAxis.text(xy[0], xy[1], display_txt, bbox={\r\n",
        "                         'facecolor': color, 'alpha': 0.5})\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEP88ZrNBxQd"
      },
      "source": [
        "class SSDPredictShow():\r\n",
        "    \"\"\"SSDでの予測と画像の表示をまとめて行うクラス\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, img_list, dataset,  eval_categories, net=None, dataconfidence_level=0.6):\r\n",
        "        self.img_list = img_list\r\n",
        "        self.dataset = dataset\r\n",
        "        self.net = net\r\n",
        "        self.dataconfidence_level = dataconfidence_level\r\n",
        "        self.eval_categories = eval_categories\r\n",
        "\r\n",
        "    def show(self, img_index, predict_or_ans):\r\n",
        "        \"\"\"\r\n",
        "        物体検出の予測と表示をする関数。\r\n",
        "\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        img_index:  int\r\n",
        "            データセット内の予測対象画像のインデックス。\r\n",
        "        predict_or_ans: text\r\n",
        "            'precit'もしくは'ans'でBBoxの予測と正解のどちらを表示させるか指定する\r\n",
        "\r\n",
        "        Returns\r\n",
        "        -------\r\n",
        "        なし。rgb_imgに物体検出結果が加わった画像が表示される。\r\n",
        "        \"\"\"\r\n",
        "        rgb_img, true_bbox, true_label_index, predict_bbox, pre_dict_label_index, scores = ssd_predict(img_index, self.img_list,\r\n",
        "                                                                 self.dataset,\r\n",
        "                                                                 self.net,\r\n",
        "                                                                 self.dataconfidence_level)\r\n",
        "\r\n",
        "        if predict_or_ans == \"predict\":\r\n",
        "            vis_bbox(rgb_img, bbox=predict_bbox, label_index=pre_dict_label_index,\r\n",
        "                     scores=scores, label_names=self.eval_categories)\r\n",
        "\r\n",
        "        elif predict_or_ans == \"ans\":\r\n",
        "            vis_bbox(rgb_img, bbox=true_bbox, label_index=true_label_index,\r\n",
        "                     scores=None, label_names=self.eval_categories)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXXwNddKBy6W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycW7Y3nAB0sJ"
      },
      "source": [
        "推論を実行する\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axKIZhbTB2YV"
      },
      "source": [
        "from utils.ssd_model import make_datapath_list, VOCDataset, DataTransform, Anno_xml2list, od_collate_fn\r\n",
        "\r\n",
        "\r\n",
        "# ファイルパスのリストを取得\r\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\r\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\r\n",
        "    rootpath)\r\n",
        "\r\n",
        "# Datasetを作成\r\n",
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\r\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair',\r\n",
        "               'cow', 'diningtable', 'dog', 'horse',\r\n",
        "               'motorbike', 'person', 'pottedplant',\r\n",
        "               'sheep', 'sofa', 'train', 'tvmonitor']\r\n",
        "color_mean = (104, 117, 123)  # (BGR)の色の平均値\r\n",
        "input_size = 300  # 画像のinputサイズを300×300にする\r\n",
        "\r\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"val\", transform=DataTransform(\r\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\r\n",
        "\r\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\r\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Moa1-ZdIB2yn"
      },
      "source": [
        "from utils.ssd_model import SSD\r\n",
        "\r\n",
        "# SSD300の設定\r\n",
        "ssd_cfg = {\r\n",
        "    'num_classes': 21,  # 背景クラスを含めた合計クラス数\r\n",
        "    'input_size': 300,  # 画像の入力サイズ\r\n",
        "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\r\n",
        "    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\r\n",
        "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\r\n",
        "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\r\n",
        "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\r\n",
        "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\r\n",
        "}\r\n",
        "\r\n",
        "# SSDネットワークモデル\r\n",
        "net = SSD(phase=\"inference\", cfg=ssd_cfg)\r\n",
        "net.eval()\r\n",
        "\r\n",
        "# SSDの学習済みの重みを設定\r\n",
        "net_weights = torch.load(weights_dir+'/ssd300_50.pth',\r\n",
        "                         map_location={'cuda:0': 'cpu'})\r\n",
        "\r\n",
        "#net_weights = torch.load(weights_dir+'/ssd300_mAP_77.43_v2.pth',\r\n",
        "#                         map_location={'cuda:0': 'cpu'})\r\n",
        "\r\n",
        "net.load_state_dict(net_weights)\r\n",
        "\r\n",
        "# GPUが使えるかを確認\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(\"使用デバイス：\", device)\r\n",
        "\r\n",
        "print('ネットワーク設定完了：学習済みの重みをロードしました')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mplUiUO7B4fe"
      },
      "source": [
        "# 結果の描画\r\n",
        "ssd = SSDPredictShow(img_list=train_img_list, dataset=train_dataset, eval_categories=voc_classes,\r\n",
        "                     net=net, dataconfidence_level=0.6)\r\n",
        "img_index = 0\r\n",
        "ssd.show(img_index, \"predict\")\r\n",
        "ssd.show(img_index, \"ans\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFOj9w_SB7In"
      },
      "source": [
        "# 結果の描画\r\n",
        "ssd = SSDPredictShow(img_list=val_img_list, dataset=val_dataset, eval_categories=voc_classes,\r\n",
        "                     net=net, dataconfidence_level=0.6)\r\n",
        "img_index = 0\r\n",
        "ssd.show(img_index, \"predict\")\r\n",
        "ssd.show(img_index, \"ans\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}